import os
import logging
from datetime import datetime, timedelta
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from dotenv import load_dotenv
from lib_news import fetch_news, extract_article_text
from lib_keyword import assess_relevance

logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
load_dotenv()

# 1. Load environment variables for Naver News API
user_id = os.getenv("X-Naver-Client-Id")
password = os.getenv("X-Naver-Client-Secret")

# 2. ìƒˆë¡œìš´ ë‚ ì§œ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
today = datetime.now()
from_date = today - timedelta(days=7)

# 3. ìš”ì•½ ëª¨ë¸ ë¡œë”© - Copilot ì¶”ê°€ìˆ˜ì • 2025.
tokenizer = AutoTokenizer.from_pretrained("gogamza/kobart-summarization")
model = AutoModelForSeq2SeqLM.from_pretrained("gogamza/kobart-summarization")
summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)

# 6. ê¸°ì‚¬ ìš”ì•½ - íŒŒë¼ë¯¸í„° ì¡°ì • 2025.06
def summarize_text(text):
    if not text or len(text.strip()) < 64 :
        return "ìš”ì•½í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
    try:
        return summarizer(text[:2048], max_length=128, min_length=30, do_sample=False)[0]['summary_text']
    except Exception as e:
        logging.warning(f"ìš”ì•½ ì‹¤íŒ¨: {e}")
        return "ìš”ì•½ ì‹¤íŒ¨"

# 7. HTML ìƒì„±
def create_post(articles):
    html = f"<html><head><meta charset='utf-8'><title>ë¹ˆì§‘ ë‰´ìŠ¤ ìš”ì•½ â€“ {today.strftime('%Y-%m-%d')}</title>\n"
    html += """<style>
        body { font-family: sans-serif; line-height: 1.6; max-width: 700px; margin: auto; padding: 2em; }
        h1 { color: #333; }
        article { margin-bottom: 2em; }
        footer { color: #777; font-size: 0.9em; }
    </style></head><body>"""
    html += f"<h1>ğŸ“Œ ë¹ˆì§‘ ê´€ë ¨ ë‰´ìŠ¤ ìš”ì•½ ({from_date.strftime('%Y.%m.%d')}~{today.strftime('%Y.%m.%d')})</h1>\n"

    keyword = "ë¹ˆì§‘(ì‹œê³¨ì´ë‚˜ ë„ì‹œì— ë°©ì¹˜ë˜ì–´ ì‚¬íšŒì ìœ¼ë¡œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¹ˆì§‘, slum, ghetto, vacant house)"

    if not articles:
        html += "<p>ì§€ë‚œ 1ì£¼ì¼ê°„ â€˜ë¹ˆì§‘â€™ ê´€ë ¨ ì£¼ìš” ë³´ë„ëŠ” ì•„ì§ ì—†ìŠµë‹ˆë‹¤.</p>\n"
    else:
        for art in articles:
            link = art.get('link')
            print(link)

        # 7.1 ë§í¬ í•„í„°ë§ - íŠ¹ì´ì¼€ì´ìŠ¤ ì œì™¸, ê²½ì¸ì¼ë³´ ì œì™¸2025.06  
            if "n.news" in link or "news.ifm.kr" in link or "www.dnews.co.kr" in link:
                print(link + ":ëª¨ë°”ì¼ ë˜ëŠ” í”„ë¡ì‹œë§í¬ë¡œ ë³¸ë¬¸ì´ ìˆ¨ê²¨ì ¸ ìˆìŒ.(n.newë¡œ ì‹œì‘ ë˜ëŠ” news.ifm.kr)-SKIP")
                continue
            
            title = art.get('title').replace("<b>", "").replace("</b>", "")

            relevance = assess_relevance(keyword, title)
            if relevance < 50:
                logging.info(f"ì œëª© '{title}' ì€(ëŠ”) ì—°ê´€ì„±({relevance})ì´ ë‚®ìŒ-SKIP")
                continue

            body = extract_article_text(link)
            summary = summarize_text(body)

            html += "<article>\n"
            html += f"<h5><a href='{link}' target='_blank'>{title}</a></h5>\n"
            html += f"<p><strong>ìš”ì•½:</strong> {summary}</p>\n"
            html += "</article><hr>\n"

    html += "<footer>ìë™ ìƒì„±ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.</footer>\n"
    html += "</body></html>"
    return html

# main.
if __name__ == "__main__":
    logging.info("Data Collecting ...")
    news_items = fetch_news("ë¹ˆì§‘", user_id, password, from_date)
    logging.info(f"Number of Data : {len(news_items)}")
    post_html = create_post(news_items)
    with open("result.html", "w", encoding="utf-8") as f:
        f.write(post_html)
    logging.info("Finish")