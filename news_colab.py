import unicodedata
import requests
import logging
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from newspaper import Article
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

from dotenv import load_dotenv
import os

load_dotenv()  # .env íŒŒì¼ ì½ê¸°

user_id = os.getenv("X-Naver-Client-Id")
password = os.getenv("X-Naver-Client-Secret")

# 1. ë‚ ì§œ ê³„ì‚°
today = datetime.now()
week_ago = today - timedelta(days=7)

# 2. ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')

# 3. ìš”ì•½ ëª¨ë¸ ë¡œë”© - Copilot ì¶”ê°€ìˆ˜ì • 2025.06
tokenizer = AutoTokenizer.from_pretrained("gogamza/kobart-summarization")
model = AutoModelForSeq2SeqLM.from_pretrained("gogamza/kobart-summarization")
summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)

# 4. ë‰´ìŠ¤ API í˜¸ì¶œ - Naverê°€ì… ë° API key ì…ë ¥ 2025.06
def fetch_news(query="ë¹ˆì§‘"):
    api_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": user_id,
        "X-Naver-Client-Secret": password
    }
    params = {
        "query": query,
        "sort": "date",
        "display": 30  # ìµœëŒ€ 100ê¹Œì§€ ê°€ëŠ¥
    }
    try:
        resp = requests.get(api_url, headers=headers, params=params)
        resp.raise_for_status()
        items = resp.json().get("items", [])
        filtered = []
        for item in items:
            pub_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S %z').replace(tzinfo=None)
            if pub_date >= week_ago:
                filtered.append(item)
        return filtered
    except requests.exceptions.RequestException as e:
        logging.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# 5. ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
def extract_article_text(url):
    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()
        
        if ("ë‹¤ì„¸ëŒ€ì£¼íƒ ë¹ˆì§‘ í„¸ì´ 40ëŒ€ êµ¬ì†" in article.title) :
            print("URL:  ",  url)
            print("TITLE:  ",  article.title)
            print("TEXT:  ",   article.text)
            print(article)
#5.1 clean_text(text) - 2025.06 - ì‚­ì œí•˜ê³  return article.textë¡œ ë³€ê²½í•´ë„ ë¬¸ì œ ì—†ìŒ
        article.text = unicodedata.normalize("NFKC", article.text)
        article.text = article.text.replace("\x00", "")  # Null ë¬¸ì ì œê±°
        return article.text.strip()
    except Exception as e:
        logging.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        return ""

# 6. ê¸°ì‚¬ ìš”ì•½ - íŒŒë¼ë¯¸í„° ì¡°ì • 2025.06
def summarize_text(text):
    if not text or len(text.strip()) < 64 :
        return "ìš”ì•½í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
    try:
        #return summarizer(text[:1000], max_length=100, min_length=30, do_sample=False)[0]['summary_text']
        return summarizer(text[:2048], max_length=128, min_length=30, do_sample=False)[0]['summary_text']
    except Exception as e:
        logging.warning(f"ìš”ì•½ ì‹¤íŒ¨: {e}")
        return "ìš”ì•½ ì‹¤íŒ¨"

# 7. HTML ìƒì„±
def create_post(articles):
    html = f"<html><head><meta charset='utf-8'><title>ë¹ˆì§‘ ë‰´ìŠ¤ ìš”ì•½ â€“ {today.strftime('%Y-%m-%d')}</title>\n"
    html += """<style>
        body { font-family: sans-serif; line-height: 1.6; max-width: 700px; margin: auto; padding: 2em; }
        h1 { color: #333; }
        article { margin-bottom: 2em; }
        footer { color: #777; font-size: 0.9em; }
    </style></head><body>"""
    html += f"<h1>ğŸ“Œ ë¹ˆì§‘ ê´€ë ¨ ë‰´ìŠ¤ ìš”ì•½ ({week_ago.strftime('%Y.%m.%d')}~{today.strftime('%Y.%m.%d')})</h1>\n"

    if not articles:
        html += "<p>ì§€ë‚œ 1ì£¼ì¼ê°„ â€˜ë¹ˆì§‘â€™ ê´€ë ¨ ì£¼ìš” ë³´ë„ëŠ” ì•„ì§ ì—†ìŠµë‹ˆë‹¤.</p>\n"
    else:
        for art in articles:
            title = art.get('title').replace("<b>", "").replace("</b>", "")
            link = art.get('link')
            print(link)
# 7.1 ë§í¬ í•„í„°ë§ - íŠ¹ì´ì¼€ì´ìŠ¤ ì œì™¸, ê²½ì¸ì¼ë³´ ì œì™¸2025.06  
            if "n.news" in link or "news.ifm.kr" in link:
                print(link + "ëŠ” ëª¨ë°”ì¼ìš© ë˜ëŠ” í”„ë¡ì‹œ ë§í¬ë¡œ ë³¸ë¬¸ì´ ìˆ¨ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤(n.new ë¡œ ì‹œì‘ í•„í„°ë§), ë˜ëŠ” news.ifm.kr")
                continue
            summary = "ìš”ì•½ ì‹¤íŒ¨"
            body = extract_article_text(link)
            summary = summarize_text(body)

            html += "<article>\n"
            html += f"<h5><a href='{link}' target='_blank'>{title}</a></h5>\n"
            html += f"<p><strong>ìš”ì•½:</strong> {summary}</p>\n"
            html += "</article><hr>\n"

    html += "<footer>ìë™ ìƒì„±ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.</footer>\n"
    html += "</body></html>"
    return html

# 8. ì‹¤í–‰
if __name__ == "__main__":
    logging.info("ğŸ“¥ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    news_items = fetch_news()
    logging.info(f"ğŸ“‘ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(news_items)}")
    post_html = create_post(news_items)
    with open("news.html", "w", encoding="utf-8") as f:
        f.write(post_html)
    logging.info("ìƒì„± ì™„ë£Œ")